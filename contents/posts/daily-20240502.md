---
title: "AI의 조용한 날, 그러나 지속적인 발전이 이어지다"
description: "AI의 조용한 날, 그러나 지속적인 발전이 이어지다"
date: 2024-05-02
update: 2024-05-02
tags:
  - Anthropic iOS 앱
  - LLM 모델 성능
  - AI 기술 발전
---

오늘은 AI 분야에서 비교적 조용한 하루였지만, **Anthropic**은 OpenAI에 이어 팀 계획과 iOS 앱을 [발표했습니다](https://twitter.com/AnthropicAI/status/1785685692988940509). 또한, Perplexity는 Discord를 통해 접근할 수 있는 비공개 Pages 기능을 예고하고 있습니다. ![image](https://assets.buttondown.email/images/a0e5bbe1-834d-47a4-9875-959a11ef56e1.png?w=960&fit=max)

## Today's Preview
* **LLM 모델과 프레임워크**
  - Command-R 35B 모델은 창의적 글쓰기에서 [Goliath-120과 Miqu-120 모델을 능가합니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgv10e/commandr_35b_is_incredible_for_creative_writing/). 적절한 프롬프트 사용이 중요합니다.
  - Llama-3 8B 모델은 [1백만 토큰의 컨텍스트 윈도우를 사용할 수 있습니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgzu2a/llama3_8b_256k_context_exl2_quants/). 이는 긴 컨텍스트 이해 작업에서 성능을 향상시킵니다.
  - TensorRT-LLM은 [동일 하드웨어에서 llama.cpp보다 30-70% 빠릅니다](https://jan.ai/post/benchmarking-nvidia-tensorrt-llm).
  - GPT2-Chat은 [GPT 4-Turbo보다 더 나은 추론 능력을 가질 수 있습니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgp7gi/lmsys_org_constantly_compares_new_gpt2_and_claude/).
* **AI 에이전트와 로보틱스**
  - 자가 학습 Llama-3 음성 에이전트 [데모가 공개되었습니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgtmuy/selflearning_llama3_voice_agent_with_function/).
* **AI 윤리 및 거버넌스**
  - 워싱턴에서의 AI 로비 활동은 [대형 기술 회사들이 주도하고 있습니다](https://time.com/6972134/ai-lobbying-tech-policy-surge/).
* **AI 연구**
  - DeepMind의 AlphaZero는 [9시간 만에 최고의 체스 플레이어가 되었습니다](https://twitter.com/tsarnick/status/1785050900647862683).

### AI Viewpoint 🤖
> 오늘의 AI 분야는 큰 소식은 없었지만, Anthropic의 iOS 앱 출시와 같은 지속적인 발전이 있었습니다. 이러한 소규모 발표들이 AI 기술의 미래에 어떤 영향을 미칠지 주목하는 것이 중요합니다. 특히, LLM 모델의 성능 향상과 새로운 기능들은 향후 AI의 활용도를 높이는 데 큰 역할을 할 것입니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **CUDA C++ 최적화 인사이트**: 개발자들은 CUDA C++ 코어 라이브러리의 모범 사례가 성능 향상을 가져왔다고 공유했습니다. ([CUDA C++ Core Libraries](https://twitter.com/marksaroufim/status/1785462414852714954))
- **Triton의 블록 크기 및 디버깅 문제 해결**: Triton의 최대 블록 크기는 하드웨어 제약에 의해 제한되지 않으며, 최근 인터프리터 버그 수정을 활용하기 위해 'triton-nightly' 사용이 권장됩니다. ([Triton debugging lecture](https://www.youtube.com/watch))
- **희소성 알고리즘의 벤치마킹 및 학습 촉진**: 활성화 희소성을 활용하는 알고리즘에 대한 논의가 활발히 이루어졌습니다.

**2. 인공지능 기술의 향후 전망**
- **ROCm과 Torch Nightly에 대한 토론**: AMD의 ROCm 플랫폼 사용자들은 Torch Nightly를 선호하며, 최신 버전 2.0의 Flash Attention이 AMD 포크에 포함되지 않은 이유에 대해 의문을 제기했습니다. ([tutorial resource on AMD HIP](https://www.youtube.com/playlist))
- **GPU 효율성에 대한 관심 증가**: A4000 16GB GPU는 효율성 면에서 높은 평가를 받으며, 비용 효율성 면에서 A100과 비교할 때 높은 점수를 받았습니다.
- **기술적 문제에 대한 토론**: LoRA 대 QLoRA 사용에 대한 토론에서 QLoRA가 VRAM 사용량을 75% 줄일 수 있지만 모델 정확도에서 1-2%의 손실이 발생할 수 있다는 점이 밝혀졌습니다.

**3. 모델 훈련의 혁신**
- **모델 훈련 중 양자화 문제**: `llama.cpp`에서 양자화와 관련된 문제가 보고되었으며, GitHub 이슈 [#3759](https://github.com/ollama/ollama/issues/3759) 및 [#4180](https://github.com/vllm-project/vllm/issues/4180)에서 논의되었습니다.
- **AI 개발 로드맵 제안**: AI 프로젝트 로드맵의 중요성이 강조되었으며, 작은 모델을 사용하여 대화 기술을 향상시키는 방안이 탐색되고 있습니다.

**4. 데이터 세트와 도구를 통한 AI 혁신**
- **새로운 Wikipedia RAG 데이터 세트 출시**: 다국어 훈련 데이터를 합성하기 위해 LLM을 활용하는 연구와 함께 출시되었습니다. ([here](https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7))
- **Pydantic의 통합과 Logfire의 도입**: 코드 관찰을 단순화하는 Logfire 플랫폼이 소개되었습니다. ([here](https://pydantic.dev/logfire))

**5. 가상 시뮬레이션의 진보**
- **산업 및 음악 시뮬레이터 출시**: CompSimulator와 Snow Singer Simulator가 AI 기반의 몰입형 경험을 제공하기 위해 출시되었습니다.
- **AI 애니메이션에 대한 대화**: AI 기반 애니메이션에 대한 특별한 Discord 그룹 초대가 있었습니다. ([community link](https://discord.gg/deforum))

이러한 동향과 전망들은 인공지능 기술의 발전 방향과 그 영향을 이해하는 데 중요한 정보를 제공합니다.
    
