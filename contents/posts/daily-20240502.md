---
title: "AI의 조용한 날, 그러나 변화는 계속된다"
description: "AI의 조용한 날, 그러나 변화는 계속된다"
date: 2024-05-02
update: 2024-05-02
tags:
  - Anthropic iOS app
  - LLM 모델 벤치마크
  - AI 윤리 및 거버넌스
---
    
    
오늘은 AI 분야에서 비교적 조용한 하루였으나, Anthropic은 OpenAI에 이어 4개월 만에 팀 플랜과 iOS 앱을 [발표했습니다](https://twitter.com/AnthropicAI/status/1785685692988940509?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666). 또한, Perplexity는 Discord를 통해 접근할 수 있는 개인 Pages 기능을 예고하고 있습니다. 이 외에도 다양한 AI 관련 업데이트가 있었습니다.

## Today's Preview
* **LLM 모델 및 프레임워크**
  - Command-R 35B 모델은 창의적 글쓰기에서 [Goliath-120과 Miqu-120 모델을 능가합니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgv10e/commandr_35b_is_incredible_for_creative_writing/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666).
  - Llama-3 8B 모델은 [1백만 토큰의 컨텍스트 윈도우를 사용할 수 있습니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgzu2a/llama3_8b_256k_context_exl2_quants/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666).
  - TensorRT-LLM은 [동일 하드웨어에서 llama.cpp보다 30-70% 빠릅니다](https://jan.ai/post/benchmarking-nvidia-tensorrt-llm?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666).
* **AI 에이전트 및 로보틱스**
  - 자가 학습 Llama-3 음성 에이전트 [데모가 Jetson Orin에서 실행됩니다](https://www.reddit.com/r/LocalLLaMA/comments/1cgtmuy/selflearning_llama3_voice_agent_with_function/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666).
* **AI 윤리 및 거버넌스**
  - 워싱턴에서의 AI 로비 활동은 [주로 대형 기술 회사들에 의해 지배됩니다](https://time.com/6972134/ai-lobbying-tech-policy-surge/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666).

### AI Viewpoint 🤖
> AI 분야에서는 항상 무언가 진행되고 있으며, 오늘과 같이 조용한 날에도 여러 기업들이 새로운 기능과 업데이트를 발표하고 있습니다. 이러한 발표들은 AI 기술의 지속적인 발전을 보여주며, 특히 모바일 앱과 팀 플랜과 같은 새로운 플랫폼의 출시는 사용자 경험을 향상시키고, AI의 접근성을 더욱 높일 것입니다. 또한, AI 윤리와 거버넌스에 대한 논의는 기술의 발전이 우리 사회에 미치는 영향을 균형 있게 고려하는 것이 중요함을 강조합니다.

# Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **CUDA C++ 최적화 통찰력**: 개발자들은 CUDA C++ Core Libraries의 모범 사례가 성능 향상을 가져왔다고 공유했습니다. 하지만 슬라이드를 위한 Google Drive 링크는 비어 있었습니다. 더 정확한 CUDA 커널 프로파일링 기술에 대해서도 논의되었고, NVIDIA 도구인 `nsight compute`와 `nsight system`이 `cudaEventRecord`보다 오버헤드가 적고 더 견고한 프로파일링을 제공한다고 선호됩니다. ([CUDA C++ Core Libraries](https://twitter.com/marksaroufim/status/1785462414852714954?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666))
- **Triton의 블록 크기와 디버깅 문제 해결**: Triton 영역에서 엔지니어들은 Triton의 최대 블록 크기가 CUDA와 같은 하드웨어 제약에 의해 제한되지 않음을 명확히 했습니다. ([Triton debugging lecture](https://www.youtube.com/watch?v=DdTsX6DQk24&utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666))
- **희소성 알고리즘이 벤치마크와 학습을 촉진**: AI 열성자들은 배치 크기가 1인 활성화 희소성을 활용하는 알고리즘에 대해 논의했습니다. 알고리즘의 창시자가 새로운 벤치마크와 속도/품질의 트레이드오프에 대한 통찰력을 공유하기로 약속했습니다.

**2. 인공지능 기술의 향후 전망**
- **AMD의 ROCm 및 Torch Nightly 토론**: AMD ROCm 플랫폼에 집중하는 사용자들은 Torch Nightly를 Torch 2.3보다 선호하는 경향을 나타냈습니다. 최신 버전 2.0의 flash attention이 AMD 포크에 없는 이유에 대한 질문이 있었습니다. ([AMD HIP Tutorial](https://www.youtube.com/playlist?list=PLB1fSi1mbw6IKbZSPz9a2r2DbnHWnLbF-&utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666))
- **LLM의 성능 및 실용성 향상**: 대화는 5.5비트 당 가중치가 큰 언어 모델의 성능 손실이 중요해지기 전의 임계값임을 지적했습니다. 새로운 **Hermes 2 Pro Llama 3**은 특정 작업을 잊어버리는 동시에 새로운 작업을 수행할 수 있는 기능을 획득했으며, 커뮤니티는 긴 컨텍스트 길이와 고급 토큰화 메커니즘의 통합을 최적화하는 방법을 탐구하고 있습니다.

**3. 데이터 세트 및 도구를 통한 AI 혁신**
- **새로운 Wikipedia RAG 데이터 세트 출시**: 이는 LLM을 활용하여 다국어 훈련 데이터를 합성하는 연구와 병행됩니다. ([Wikipedia RAG dataset](https://huggingface.co/collections/nthakur/swim-ir-dataset-662ddaecfc20896bf14dd9b7?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-2666))
    
    