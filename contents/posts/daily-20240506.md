---
title: "DeepSeek V2, 전문가 수를 두 배로 늘리며 Mixtral을 압도하다"
description: "DeepSeek V2, 전문가 수를 두 배로 늘리며 Mixtral을 압도하다"
date: 2024-05-06
update: 2024-05-06
tags:
  - DeepSeek V2
  - 전문가 수
  - MoE 모델
---

**전문가가 전부일까요?**

[DeepSeek V2](https://github.com/deepseek-ai/DeepSeek-V2)는 지난달 [Mistral Convex Hull](https://buttondown.email/ainews/archive/ainews-mixtral-8x22b-instruct-defines-frontier/)을 뛰어넘었습니다:

![image](https://assets.buttondown.email/images/bcf759e8-0ca7-4ccd-a901-6289aedd96ea.png?w=960&fit=max)

데이터셋에 대한 정보는 매우 제한적이지만, DeepSeek v1보다 4배 많은 8B 토큰을 사용하며, 중국어가 영어보다 약 12% 더 많다고 합니다.

[Snowflake Arctic](https://buttondown.email/ainews/archive/ainews-snowflake/)은 이전에 본 가장 큰 MoE 모델로, 전문가 수가 128명이었습니다; DeepSeek v2는 이제 DeepSeekMOE를 확장하고 새로운 주의 변형인 Multi-Head Latent Attention을 도입하면서 새로운 최고 기록을 세웠습니다.

![image](https://assets.buttondown.email/images/16916531-1d7f-4068-a398-00d74c9e8fbc.png?w=960&fit=max)

이러한 변화는 압축된 KVs를 캐싱함으로써 추론 속도를 훨씬 빠르게 합니다('KV 캐시를 93.3% 줄임').

![image](https://assets.buttondown.email/images/4b75f5cc-73a5-4525-ac1d-a394849e4cb4.png?w=960&fit=max)

논문은 다른 유용한 소소한 기법들에 대해서도 자세히 설명하고 있습니다.

DeepSeek는 실제로 투자를 하고 있습니다 - 그들은 [플랫폼에서 토큰 추론을 제공하고 있으며, 가격은 백만 토큰당 $0.28](https://twitter.com/deepseek_ai/status/1787478994478321872)으로, 2023년 12월 [Mixtral 가격 전쟁](https://twitter.com/swyx/status/1744467383090372743)에서 본 가장 낮은 가격의 절반입니다.

## Today's Preview
* **LLM 개발 및 출시**
  - **Llama 3 출시**: [@erhartford](https://twitter.com/erhartford/status/1787050962114207886)는 Llama 3 120B가 Opus보다 똑똑하며 llama3-400b에 대해 매우 기대하고 있습니다. [@maximelabonne](https://twitter.com/maximelabonne/status/1787401780021649911)은 Llama 3 120B가 GPT-4보다 창의적인 글쓰기에서 우수하지만 L3 70B보다 추론에서는 떨어진다고 공유했습니다.
  - **DeepSeek-V2 출시**: [@deepseek_ai](https://twitter.com/deepseek_ai/status/1787478986731429933)는 DeepSeek-V2, 오픈 소스 MoE 모델을 출시하여 AlignBench에서 상위 3위를 차지하며 GPT-4를 능가했습니다. 이 모델은 236B 파라미터를 가지고 있으며 생성 중에 21B가 활성화됩니다.
  - **MAI-1 500B from Microsoft**: [@bindureddy](https://twitter.com/bindureddy/status/1787498838024139185)는 Microsoft가 자체 500B 파라미터 LLM인 MAI-1을 훈련 중이며, 이는 OpenAI의 GPT 라인과 경쟁할 것으로 예상됩니다.
  - **Mistral 및 Open LLMs 과적합 벤치마크**: [@adcock_brett](https://twitter.com/adcock_brett/status/1787151286305017966)는 Scale AI가 Mistral 및 Phi와 같은 일부 LLM의 인기 있는 AI 벤치마크에서 '과적합'을 밝혀냈으며, GPT-4, Claude, Gemini, Llama는 견고함을 유지했다고 공유했습니다.

### AI Viewpoint 🤖
> DeepSeek V2의 출시는 AI 분야에서 전문가의 수를 늘리는 것이 얼마나 중요한지를 보여줍니다. 이 모델은 기존의 성공적인 접근 방식을 확장하고 새로운 주의 메커니즘을 도입하여 더 빠른 추론 속도와 비용 효율성을 실현하고 있습니다. 이러한 발전은 AI 기술의 미래에 큰 영향을 미칠 것입니다. 또한, DeepSeek V2는 경쟁적인 가격 설정으로 시장에서의 위치를 강화하고 있으며, 이는 다른 기업들에게도 중요한 시사점을 제공합니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **Triton과 CUDA의 한계와 협업**: 엔지니어들은 Triton 블록의 한계를 논의하며, 4096 요소 블록은 실행 가능하지만 8192 요소 블록은 실행 불가능함을 확인했습니다. 이는 예상 CUDA 한계와의 불일치를 시사합니다. ([Compiler Explorer](https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **PyTorch와 LLM 추론 최적화**: PyTorch의 `linear` 함수와 행렬 곱셈 커널의 동작을 검토하고, Effort Engine 알고리즘이 LLM 추론 중 계산 노력을 조정할 수 있게 함으로써, Apple Silicon에서 표준 행렬 곱셈과 비슷한 속도를 낼 수 있음을 논의했습니다. ([Effort Engine on kolinko](https://kolinko.github.io/effort?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408), [GitHub](https://github.com/kolinko/effort?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **InstaDeep의 기계학습 엔지니어 채용**: InstaDeep은 고성능 ML 엔지니어링, 사용자 정의 CUDA 커널 및 분산 훈련에 능숙한 기계 학습 엔지니어를 채용 중입니다. ([InstaDeep Careers](https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))

**2.인공지능 기술의 향후 전망**
- **LLM의 컨텍스트 길이 증가**: Llama-3 8B 모델이 컨텍스트 길이 증가를 통해 새로운 벤치마크를 설정했습니다. 이는 LLM의 가능성을 크게 확장시키는 발전입니다. ([Llama-3 8B Gradient Instruct 1048k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **ROCm과 Flash Attention 2의 적응**: ROCm 채널에서는 NVIDIA의 Flash Attention 2를 ROCm에 적용하는 논의가 있었습니다. 이는 ROCm 6.x 버전과의 호환성에 중점을 두고 있습니다. ([ROCm/flash-attention on GitHub](https://github.com/ROCm/flash-attention?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
    
