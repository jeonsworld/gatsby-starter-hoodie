---
title: "구글 I/O의 혁신적 AI 모델, 젬마와 젬마니의 새로운 맛을 찾아라!"
description: "구글 I/O의 혁신적 AI 모델, 젬마와 젬마니의 새로운 맛을 찾아라!"
date: 2024-05-14
update: 2024-05-14
tags:
  - 젬마니 모델 패밀리
  - AI 기술 발표
  - 구글 I/O
---

구글 I/O는 여전히 진행 중이며, OpenAI의 반시간 행사보다 훨씬 더 많은 제품 범위로 인해 커버하기가 어렵습니다. 아직 모든 것을 요약한 단일 웹페이지는 없습니다([@Google](https://twitter.com/google)과 [@OfficialLoganK](https://twitter.com/OfficialLoganK/) 계정을 제외하고). 여기 주관적으로 정리된 목록이 있습니다:

**젬마니 모델 패밀리**:
- **젬마니 1.5 프로**가 발표되었으며, **2m 토큰 지원**을 제공합니다(대기 목록 중). [블로그 포스트](https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/)에서는 '번역, 코딩, 추론 등 주요 사용 사례에 걸쳐 일련의 품질 개선'을 언급했지만 벤치마크는 발표하지 않았습니다.
- **젬마니 플래시**가 추가되어 [원래의 3모델 비전](https://arxiv.org/abs/2312.11805)에 네 번째 모델이 추가되었습니다. 이 블로그 포스트는 모델의 응답 시간이 가장 중요한 좁거나 빈번한 작업에 최적화되었다고 언급하며, [1m 토큰 용량](https://x.com/Google/status/1790432952767115432)을 [GPT3.5보다 약간 저렴한 가격](https://x.com/_Mira___Mira_/status/1790448070226030920)에 제공하지만 [속도 주장은 제공하지 않습니다](https://news.ycombinator.com/item). 젬마니 스위트는 이제 다음과 같습니다:
  - 울트라: '가장 큰 모델'(오직 [젬마니 어드밴스드](https://techcrunch.com/2024/02/08/google-goes-all-in-on-gemini-and-launches-20-paid-tier-for-gemini-ultra/)에서만 사용 가능)
  - 프로: '일반 성능에 가장 적합한 모델'(오늘 API 미리보기에서 사용 가능, 6월에 일반 출시)
  - 플래시: '속도/효율성을 위한 경량 모델'(오늘 API 미리보기에서 사용 가능, 6월에 일반 출시)
  - 나노: '기기 내 모델'( [크롬 126](https://techcrunch.com/2024/05/14/google-is-building-its-gemini-nano-ai-model-into-chrome-on-the-desktop/)에 탑재될 예정)
- [젬마니 젬스](https://x.com/Google/status/1790444941451067901) - 젬마니 버전의 맞춤형 GPT
- [젬마니 라이브](https://x.com/Google/status/1790444519864795458): '음성을 사용하여 심층적인 양방향 대화를 할 수 있는 기능.', 이는 직접 [프로젝트 아스트라](https://x.com/Google/status/1790433789811753460)로 이어집니다 - 실시간 비디오 이해 개인 비서 챗봇으로 [꼼꼼한 2분 데모](https://x.com/Google/status/1790433789811753460)가 제공됩니다.
- [LearnLM](https://x.com/Google/status/1790453655054827679) - 젬마니를 기반으로 하고 학습에 특화된 새로운 모델 패밀리

**젬마 모델 패밀리**:
- [젬마 2](https://x.com/Google/status/1790452314278412554), 이제 27B(이전에는 7B와 2B)로, 아직 훈련 중인 모델로서 라마-3-70B 성능의 절반 크기에서 제공됩니다 ![image](https://assets.buttondown.email/images/eee89aed-9b00-4e60-aeda-005b3ff69897.png?w=960&fit=max)
- [팔리젬마](https://x.com/Google/status/1790451427464085563) - [팔리-3](https://arxiv.org/abs/2310.09199)에서 영감을 받은 첫 번째 비전-언어 오픈 모델로, [코드젬마](https://ai.google.dev/gemma/docs/codegemma) 및 [리커런트젬마](https://ai.google.dev/gemma/docs/recurrentgemma)를 보완합니다.

**기타 출시**:
- [베오](https://x.com/Google/status/1790435689495945479), 딥마인드의 소라에 대한 대답. [HN에서의 비교](https://news.ycombinator.com/item).
- [이마젠 3](https://x.com/Google/status/1790434730623537280): '사람들이 작성하는 방식대로 프롬프트를 이해하고, 더 사실적인 이미지를 생성하며, 텍스트 렌더링에 가장 적합한 모델입니다.' (더 많은 [샘플은 여기](https://x.com/GoogleDeepMind/status/1790434750592643331)에서 확인할 수 있습니다.)
- [뮤직 AI 샌드박스](https://x.com/GoogleDeepMind/status/1790435413682975043) - 유튜브 x 딥마인드가 Udio/Suno와 경쟁하기 위해 협력합니다.
- [신티드 워터마킹](https://x.com/Google/status/1790453029243703658)이 이제 텍스트뿐만 아니라 이미지, 오디오 및 비디오(베오 포함)에도 확장됩니다.
- [트릴리움 - TPUv6의 코드명](https://x.com/Google/status/1790436855395078537)

그리고 구글 제품군 전반에 걸친 AI 배치 - [워크스페이스](https://x.com/Google/status/1790430549649019123), [이메일](https://x.com/Google/status/1790441491338264973), [도큐먼트](https://x.com/GoogleWorkspace/status/1790441310123385236), [시트](https://x.com/Google/status/1790442954500268164), [포토스](https://x.com/Google/status/1790428759700463632), [서치 오버뷰](https://x.com/Google/status/1790428396775719053), [멀티스텝 리즈닝을 사용한 서치](https://x.com/Google/status/1790438800667123860), [안드로이드 서클 투 서치](https://x.com/Google/status/1790447502107251189), [렌즈](https://x.com/Google/status/1790440001156583712).

전반적으로 매우 능숙하게 실행된 I/O로, 너무 많은 세부 사항을 잃지 않고 요약하기 쉽습니다. 세계는 애플의 대답을 기다리고 있습니다.

## Today's Preview
* 제목
  - 내용

### AI Viewpoint 🤖
> 이번 구글 I/O에서 발표된 다양한 AI 모델과 기술들은 AI 분야의 미래에 큰 영향을 미칠 것으로 보입니다. 특히 젬마니와 젬마 모델 패밀리의 확장은 구글이 AI 기술의 다양한 분야에서 선도적인 위치를 유지하려는 의지를 보여줍니다. 이러한 모델들은 번역, 코딩, 추론 등 다양한 사용 사례에서 품질을 개선할 가능성이 크며, 특히 젬마니 플래시와 같은 새로운 추가 모델은 특정 작업에 최적화된 솔루션을 제공함으로써 더욱 빠르고 효율적인 AI 응답을 가능하게 할 것입니다. 또한, 이러한 발표는 다른 기술 기업들, 특히 애플에게도 새로운 도전을 제기하며, 향후 몇 년간 AI 경쟁 구도에 어떤 변화가 일어날지 주목되는 부분입니다.

## Today's Overview
### 종합적인 인공지능 기술 동향 및 응용 분야 리포트

**1. GPT-4o의 새로운 기능과 시장 반응**
- **GPT-4o의 출시와 주요 특징**: OpenAI가 새로운 모델인 GPT-4o를 출시했습니다. 이 모델은 텍스트, 이미지 입력을 지원하며 곧 음성 및 비디오 기능이 추가될 예정입니다. GPT-4o는 실시간으로 오디오, 비전, 텍스트를 처리할 수 있는 능력을 갖추고 있어 멀티모달 애플리케이션에서 중요한 진전을 나타냅니다. ([자세히 보기](https://openai.com/index/hello-gpt-4o/))

- **시장 및 사용자 반응**: GPT-4o는 빠른 응답 속도와 무료 채팅 기능으로 주목을 받았습니다. 그러나 일부 사용자는 브랜딩과 성능, 특히 지연 시간에 대한 우려를 표명했습니다.

**2. Claude와 GPT-4o의 비교 및 경쟁**
- **Claude의 복잡한 작업 수행 능력**: Claude Opus는 복잡하고 장문의 추론 작업에서 GPT-4o보다 우수하다고 평가받고 있습니다. 특히 원본 콘텐츠를 광범위하게 처리할 때 더 뛰어난 성능을 보입니다.

- **사용자 기반의 선호도**: 일부 사용자는 비용과 사용 제한에도 불구하고 텍스트 요약 및 인간과 유사한 반응을 위해 Claude 3 Opus를 선호합니다.

**3. GPT-4o의 멀티모달 기능과 통합 문제**
- **멀티모달 기능의 확장**: GPT-4o는 멀티모달 기능을 강화하여 오디오 입력에 대한 whispering latents와 비영어 토큰 처리를 개선하고 있습니다. 이는 GPT-4o의 기능을 더욱 다양화하고 있습니다.

- **통합과 API 접근성**: GPT-4o의 API는 빠른 성능으로 엔지니어들 사이에서 기대를 모으고 있으며, OpenAI의 전략적 배치로 인해 Google 등 다른 경쟁 업체들도 반응하고 있습니다.

**4. 기술적 문제 및 커뮤니티의 도전**
- **모델의 이해도와 실행 문제**: GPT-4o는 창의적이고 공간적인 인식이 필요한 작업을 수행할 때 어려움을 겪고 있으며, 이는 반복적인 이미지 생성과 특정 콘텐츠 모더레이션 이슈로 이어지고 있습니다.

- **커뮤니티의 기술적 대응**: 사용자들은 GPT-3.5 모델을 사용하여 조직에서 메시지를 모니터링할 수 있는 ChatGPT와 유사한 애플리케이션을 만들고자 하는 등, 맞춤형 및 제어 가능한 AI 도구에 대한 필요성을 반영하고 있습니다.

이러한 동향과 사용자 반응은 인공지능 기술의 미래 방향성에 중요한 통찰을 제공하며, GPT-4o와 같은 신기술이 시장에 어떤 영향을 미칠지에 대한 중요한 데이터를 제공합니다.
    
