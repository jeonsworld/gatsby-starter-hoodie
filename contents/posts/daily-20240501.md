---
title: "LLM을 활용한 다중 판사 시스템의 효율성과 비용 절감"
description: "LLM을 활용한 다중 판사 시스템의 효율성과 비용 절감"
date: 2024-05-01
update: 2024-05-01
tags:
  - LLM
  - 다중 판사 시스템
  - 비용 효율성
---

최근 연구에 따르면, 단일 대형 언어 모델(Large Language Model, LLM)을 사용하는 것보다 다양한 LLM을 조합하여 사용할 경우, 데이터 평가에 있어서 더 높은 성능을 보이는 것으로 나타났습니다. 이러한 방법은 'PoLL' 방식으로 알려져 있으며, 다양한 LLM을 활용함으로써 단일 모델 사용시 발생할 수 있는 비용을 상당히 절감할 수 있습니다. 특히, GPT-4와 비교했을 때 7-8배 저렴한 비용으로 운영이 가능하다는 장점이 있습니다. [Cohere의 연구](https://twitter.com/cohere/status/1785284142789242932)에 따르면, 이러한 다중 LLM 시스템은 단일 판사 시스템보다 우수한 성능을 보여주었습니다.

## Today's Preview
* OpenAI 뉴스
  - ChatGPT Plus 사용자를 위한 메모리 기능이 모든 사용자에게 제공됩니다. [자세히 보기](https://twitter.com/OpenAI/status/1784992796669096181)
* LLMs와 AI 모델
  - Llama 3 모델은 32k 컨텍스트에서 뛰어난 품질을 보여줍니다. [자세히 보기](https://twitter.com/abacaj/status/1785147493728039111)
* 프롬프트 엔지니어링 및 평가
  - 다중 LLM 판사를 사용하는 PoLL 방식이 단일 대형 모델 판사보다 우수한 성능을 보여줍니다. [자세히 보기](https://twitter.com/cohere/status/1785284142789242932)

### AI Viewpoint 🤖
> 이 연구는 LLM의 활용 방안에 있어서 중요한 시사점을 제공합니다. 다양한 LLM을 조합함으로써 단일 모델의 한계를 극복하고 비용 효율성을 극대화할 수 있는 가능성을 보여줍니다. 특히, 법률 및 판결 분야에서의 AI 활용에 있어서, 다중 판사 시스템은 보다 공정하고 정확한 판단을 가능하게 할 것입니다. 이는 AI 기술의 발전이 단순한 기술적 진보를 넘어 사회적, 윤리적 문제에 대한 해결책을 제공할 수 있음을 시사합니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **Triton과 CUDA의 한계와 협업**: 엔지니어들은 Triton 블록의 한계를 논의하며, 4096 요소 블록은 실행 가능하지만 8192 요소 블록은 실행 불가능함을 확인했습니다. 이는 예상 CUDA 한계와의 불일치를 시사합니다. ([Compiler Explorer](https://godbolt.org/z/9K9Gf1v6P))
- **PyTorch와 LLM 추론 최적화**: PyTorch의 `linear` 함수와 행렬 곱셈 커널의 동작을 검토하고, Effort Engine 알고리즘이 LLM 추론 중 계산 노력을 조정할 수 있게 함으로써, Apple Silicon에서 표준 행렬 곱셈과 비슷한 속도를 낼 수 있음을 논의했습니다. ([Effort Engine on kolinko](https://kolinko.github.io/effort), [GitHub](https://github.com/kolinko/effort))
- **InstaDeep의 기계학습 엔지니어 채용**: InstaDeep은 고성능 ML 엔지니어링, 사용자 정의 CUDA 커널 및 분산 훈련에 능숙한 기계 학습 엔지니어를 채용 중입니다. ([InstaDeep Careers](https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f))

**2.인공지능 기술의 향후 전망**
- **LLM의 컨텍스트 길이 증가**: Llama-3 8B 모델이 컨텍스트 길이 증가를 통해 새로운 벤치마크를 설정했습니다. 이는 LLM의 가능성을 크게 확장시키는 발전입니다. ([Llama-3 8B Gradient Instruct 1048k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k))
- **ROCm과 Flash Attention 2의 적응**: ROCm 채널에서는 NVIDIA의 Flash Attention 2를 ROCm에 적용하는 논의가 있었습니다. 이는 ROCm 6.x 버전과의 호환성에 중점을 두고 있습니다. ([ROCm/flash-attention on GitHub](https://github.com/ROCm/flash-attention))
    
