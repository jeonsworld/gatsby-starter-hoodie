---
title: "Snowflake Arctic의 혁신: 10B+128x4B Dense-MoE 하이브리드 LLM의 개발"
description: "Snowflake Arctic의 혁신: 10B+128x4B Dense-MoE 하이브리드 LLM의 개발"
date: 2024-04-26
update: 2024-04-26
tags:
  - Snowflake Arctic
  - LLM
  - 오픈 소스
---
    
    
Snowflake의 최신 프로젝트인 **Snowflake Arctic**은 주목할 만한 여러 가지 이유가 있습니다. 이 모델은 **10B dense transformer**와 **128x3.66B MoE MLP**를 결합한 것으로, [Snowflake Arctic](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake)에서 자세히 설명되어 있습니다. 주요 경쟁사인 Databricks와의 비교에서 거의 모든 면에서 우위를 점하고 있다고 주장하고 있습니다. 또한, 이 모델은 **Apache 2.0**으로 공개되어 있으며, Snowflake는 이를 Medium.com에서 [Snowflake Arctic cookbook](https://medium.com/@snowflake_ai_research/snowflake-arctic-cookbook-series-exploring-mixture-of-experts-moe-c7d6b8f14d16?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake) 시리즈를 통해 자세히 소개하고 있습니다.

![Snowflake Arctic Model](https://assets.buttondown.email/images/8a45195d-2c7c-420b-a6cc-dcf124fc1d84.png?w=960&fit=max)

이 모델의 아키텍처는 [DeepSeekMOE](https://x.com/deepseek_ai/status/1745304852211839163?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake)와 [DeepSpeedMOE](https://arxiv.org/pdf/2201.05596?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake)에서 영감을 받아 더 많은 전문가가 더 나은 결과를 낼 수 있다는 점을 강조합니다.

## Today's Preview
* Snowflake Arctic의 혁신
  - Snowflake가 개발한 10B+128x4B Dense-MoE 하이브리드 LLM은 엔터프라이즈 인텔리전스를 강화하고, 경쟁사 대비 우수한 성능을 제공합니다.
* 기술적 세부사항
  - 이 모델은 DeepSeekMOE와 DeepSpeedMOE의 영향을 받아 더 많은 전문가들을 통한 향상된 결과를 도모합니다.
* 오픈 소스 접근성
  - Apache 2.0 라이선스 하에 공개되어, 더 넓은 커뮤니티와의 협력을 가능하게 합니다.
* 추가 자료
  - Snowflake는 Medium.com에서 모델의 사용법을 자세히 설명하는 콘텐츠를 제공하고 있습니다.

### AI Viewpoint 🤖
> Snowflake Arctic의 출시는 기업용 AI의 새로운 지평을 열었다고 볼 수 있습니다. 이 모델은 효율적인 학습과 뛰어난 성능을 통해 기업들이 데이터를 더 깊이 있고 정확하게 분석할 수 있는 기회를 제공합니다. 특히, 오픈 소스로 제공되어 다양한 개발자와 연구자들이 자유롭게 접근하고 개선할 수 있는 점은 매우 긍정적입니다. 하지만, 모델을 더 널리 활용하기 위해서는 사용법에 대한 더 많은 교육과 자료가 필요할 것입니다.

# Today's Overview
### 최신 인공지능 기술 동향 및 응용 분야 종합 리포트

**1. 인공지능 기술의 최신 동향**
- **Unsloth Pro의 멀티 GPU 지원 개발**: Unsloth Pro는 멀티 GPU 지원을 개발 중이며, 5월에 오픈 소스 버전 출시 예정입니다. PR #377은 모델의 어휘 크기 조정 문제를 해결하고자 하며, 커뮤니티의 큰 관심을 받고 있습니다. ([Unsloth Github PR #377](https://github.com/unslothai/unsloth/pull/377?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake))
- **LLaMA 모델의 효율적인 파인 튜닝**: LLaMA 모델의 파인 튜닝을 위해 불필요한 토큰을 제거하면 손실을 크게 줄일 수 있습니다. 최적의 파인 튜닝 설정은 최소 48GB GPU VRAM을 요구합니다.
- **AI의 두뇌 최적화**: 최적의 리랭커로 ms Marco l6 V2가 선정되어 BGE-m3보다 15배 빠른 속도를 보였습니다. PostgreSQL의 pgvector는 외부 API의 필요성을 우회합니다.

**2. 인공지능 기술의 미래 전망**
- **Meta의 LlaMA-3 시리즈 도입**: Meta는 LlaMA-3 시리즈를 도입하며 8B 및 400B 모델을 예고했습니다. 이는 GPT-4 벤치마크에 도전장을 내밀고 있습니다. ([Substack piece](https://datta0.substack.com/p/ai-unplugged-8-llama3-phi-3-training?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake))
- **기술적 팁으로 트레이닝 최적화**: Colab 노트북을 활용하여 GPT3.5 또는 GPT4를 이용한 다중 선택 문제 생성을 최적화할 수 있습니다.

**3. 주목받는 새로운 기업 및 투자**
- **Enigmagi의 대규모 투자 유치**: Enigmagi는 $62.7백만의 투자를 유치하며 $1.04억의 평가를 받았습니다. NVIDIA와 Jeff Bezos가 주요 투자자로 참여했습니다. ([Enigmagi](https://perplexity.ai?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake))

**4. 인공지능 응용 프로그램의 실제 사례**
- **Phi와 TinyLLamas의 주목**: Phi-3 모델은 LM Studio에서 실험되었으며, TinyLlamas 모델은 Hugging Face에서 주목을 받고 있습니다. ([Hugging Face](https://huggingface.co/DavidAU?utm_source=ainews&utm_medium=email&utm_campaign=ainews-snowflake))

이 외에도 다양한 인공지능 기술과 응용 분야에 대한 최신 정보와 통찰력을 제공하기 위해 지속적으로 업데이트할 예정입니다.
    
    