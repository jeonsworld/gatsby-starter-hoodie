---
title: "Snowflake Arctic: 혁신적인 10B+128x4B Dense-MoE 하이브리드 LLM의 등장"
description: "Snowflake Arctic: 혁신적인 10B+128x4B Dense-MoE 하이브리드 LLM의 등장"
date: 2024-04-26
update: 2024-04-26
tags:
  - Snowflake Arctic
  - LLM
  - AI 기술
---

Snowflake의 최신 프로젝트인 **Snowflake Arctic**는 여러 면에서 주목할 만합니다. 특히, [Snowflake Arctic](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/)는 그들의 주요 경쟁자인 Databricks와 비교하여 거의 모든 면에서 우수하다고 주장하는 차트를 통해 눈길을 끕니다. 이 차트는 Snowflake가 어떻게 LLM을 개선하여 특정 도메인에서 더 나은 성능을 보이는지 설명합니다. ![Snowflake Chart](https://assets.buttondown.email/images/1ab962bf-83de-4037-8fc1-b0a1e1bfa9d3.png?w=960&fit=max) 또한, 이 모델은 [DeepSeekMOE](https://x.com/deepseek_ai/status/1745304852211839163)와 [DeepSpeedMOE](https://arxiv.org/pdf/2201.05596)에서 영감을 받아 더 많은 전문가의 참여가 모델 성능을 향상시킨다는 점을 강조합니다. 마지막으로, 이 모델은 Apache 2.0 라이선스 하에 공개되었으며, [Snowflake Arctic cookbook](https://medium.com/@snowflake_ai_research/snowflake-arctic-cookbook-series-exploring-mixture-of-experts-moe-c7d6b8f14d16)은 Medium에서 출판되었습니다.

## Today's Preview
* AI 이미지/비디오 생성
  - **Nvidia Align Your Steps**: Nvidia의 새로운 [Align Your Steps 기술](https://www.reddit.com/gallery/1ccdt3x)은 낮은 단계에서 이미지 품질을 크게 향상시켜 적은 단계로도 좋은 품질의 이미지를 생성할 수 있게 합니다.
* 언어 모델 및 챗봇
  - **Apple 오픈 소스 AI 모델**: Apple은 [코드, 훈련 로그 및 다양한 버전](https://www.macrumors.com/2024/04/24/apple-ai-open-source-models/)의 기기 내 언어 모델을 공개했습니다.
* AI 하드웨어 및 인프라
  - **Nvidia DGX H200 for OpenAI**: Nvidia CEO는 OpenAI에 세계 최초의 DGX H200 시스템을 [전달했습니다](https://i.redd.it/wnxzyyqurhwc1.jpeg).
* AI 윤리 및 사회적 영향
  - **Deepfake Nudes 법률**: 미성년자의 AI 생성 성적 이미지와 싸우기 위해 24개 주의 입법자들이 [법안을 작업 중이거나 법을 통과시켰습니다](https://www.nytimes.com/2024/04/22/technology/deepfake-ai-nudes-high-school-laws.html).

### AI Viewpoint 🤖
> Snowflake Arctic의 출시는 기업 지능 분야에서의 혁신적 접근을 보여줍니다. 이 모델은 특히 대규모 언어 모델의 효율성과 전문성을 결합한 점에서 주목할 만하며, 이는 향후 비슷한 프로젝트에 중요한 영향을 미칠 것입니다. 또한, 이러한 모델이 어떻게 다양한 산업에 적용될 수 있는지에 대한 논의는 AI 기술의 미래 방향성에 대한 중요한 통찰력을 제공합니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **Unsloth Pro의 멀티 GPU 지원 개발**: Unsloth Pro는 멀티 GPU 지원을 개발 중이며, 5월에 오픈 소스 버전을 출시할 예정입니다. 이와 관련하여 PR #377은 모델의 어휘 크기 조정 문제를 해결하기 위한 제안으로, 커뮤니티에서 큰 기대를 모으고 있습니다. ([Unsloth GitHub PR #377](https://github.com/unslothai/unsloth/pull/377))
- **LLaMA 모델의 성능 개선**: LLaMA 모델의 프롬프트에서 불필요한 토큰을 제거함으로써 파인튜닝을 개선하는 방법이 확인되었습니다. 이는 초기 에포크에서 손실을 5에서 더 낮은 범위로 크게 줄이는 데 도움이 되었습니다.
- **AI의 두뇌력 최적화**: ms Marco l6 V2가 BGE-m3보다 15배 빠른 속도 향상을 가져왔으며, PostgreSQL의 pgvector는 외부 API의 필요성을 우회합니다. 하드웨어 측면에서는 RAG 데이터셋과 비지도 인컨텍스트 학습을 위한 새로운 프로젝트를 강화하는 데 도움이 되는 새로운 획득이 있었습니다.

**2. 인공지능 기술의 향후 전망**
- **Meta의 모델 마라톤에서의 위력**: Meta는 LlaMA-3 시리즈를 소개하며 8B 및 400B 모델로 GPT-4 벤치마크에 도전장을 내밀었습니다. 오픈 소스 AI는 LlaMA-3 및 Phi-3의 출시와 함께 탄력을 받고 있으며, 이는 [Substack 기사](https://datta0.substack.com/p/ai-unplugged-8-llama3-phi-3-training)에서 자세히 설명되어 있습니다.
- **기술적 팁으로 훈련 가속화**: Colab 노트북을 사용하여 파인튜닝 오류를 해결하고, GPT3.5 또는 GPT4를 사용하여 다중 선택 문제를 작성하는 방법이 포함됩니다. 임베딩 행렬을 희소화하고 동적으로 컨텍스트 길이를 조정하는 방법에 대한 관심도 있습니다.

**3. 투자 및 시장 동향**
- **Enigmagi의 대규모 투자 유치**: Enigmagi는 $62.7백만의 투자를 유치하여 $1.04억의 평가를 받았으며, NVIDIA와 Jeff Bezos가 투자자로 참여하였습니다. iOS 사용자를 위한 Pro 서비스도 출시되었습니다. ([Enigmagi](https://perplexity.ai))

이 리포트는 인공지능 기술의 최신 동향과 미래 전망을 종합적으로 다루며, 특히 기술 개발과 시장 동향에 중점을 두고 있습니다. 각 섹션은 독자들이 기술의 진화를 이해하고 이에 따른 전략을 수립할 수 있도록 구체적인 정보와 데이터를 제공합니다.
    
