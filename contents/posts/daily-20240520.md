---
title: "AI의 최신 동향과 혁신: 주목할 만한 모델 릴리스와 연구 논문"
description: "AI의 최신 동향과 혁신: 주목할 만한 모델 릴리스와 연구 논문"
date: 2024-05-20
update: 2024-05-20
tags:
  - AI 모델 업데이트
  - GPU 지원
  - AI 윤리
---

이번 주말은 비교적 활발했으나 대부분의 토론은 기술적인 내용이 아니었습니다. 그 대신, 몇 가지 소소한 노트를 나열해보겠습니다:

- 몇몇 비활성화된 Discord가 폐지되었고, **Hamel Husain과 Dan Becker의 새로운 LLM Fine-Tuning Discord**가 그의 인기 있는 Maven 과정을 위해 추가되었습니다. ([Maven 과정 링크](https://maven.com/parlance-labs/fine-tuning))
- **HuggingFace의 ZeroGPU**는 Hugging Face의 Spaces를 통해 제공되며, 개발자들이 새로운 AI 기술을 창출할 수 있도록 $10백만의 무료 공유 GPU를 제공합니다. ([HuggingFace의 ZeroGPU 정보](https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai))
- LangChain은 v0.2 릴리스에 이어 필요한 문서 업데이트를 진행했습니다. ([LangChain 문서 업데이트](https://blog.langchain.dev/documentation-refresh-for-langchain-v0-2/))
- Omar Sanseviero는 지난주에 출시된 작은 모델들에 대한 스레드를 공유했습니다. ([Omar Sanseviero의 스레드](https://x.com/osanseviero/status/1792273392839557288))

그러나 우리 모두가 알고 있듯이, 여러분은 아마도 Scarlett Johansson이 OpenAI를 비판하는 Apple Notes를 읽고 싶어할 것입니다. ([Scarlett의 Apple Notes](https://x.com/BobbyAllyn/status/1792679435701014908))

![이미지](https://assets.buttondown.email/images/89e806ac-a369-415c-8b42-14465dfc9877.png?w=960&fit=max)

## Today's Preview
* AI 모델 릴리스 및 업데이트
  - **Google DeepMind의 Gemini 1.5 Pro 및 Flash 모델 출시**: Gemini 1.5 Pro는 텍스트, 오디오, 이미지 및 비디오를 처리하는 희소 다중 모드 MoE 모델로 최대 10M 컨텍스트를 지원하며, Flash는 Pro에서 추출한 밀집 트랜스포머 디코더 모델로 **3배 빠르고 10배 저렴**합니다. ([@_philschmid 트윗](https://twitter.com/_philschmid/status/1792528829040251147))
  - **Yi AI의 Yi-1.5 모델, 더 긴 컨텍스트 버전 출시**: Yi-1.5 모델은 **32K 및 16K 컨텍스트 길이**를 지원하며 Hugging Face에서 사용 가능합니다. ([@01AI_Yi 트윗](https://twitter.com/01AI_Yi/status/1792386612430774510))
  - **다른 주목할 만한 모델 릴리스**: Kosmos 2.5(Microsoft), PaliGemma(Google), CumoLLM, Falcon 2, DeepSeek v2 lite, HunyuanDiT 확산 모델, Lumina next 등이 포함됩니다. ([@osanseviero 트윗](https://twitter.com/osanseviero/status/1792273392839557288))

### AI Viewpoint 🤖
> 이번 주 AI 뉴스는 다양한 모델 업데이트와 새로운 리소스의 제공으로 풍성했습니다. 특히, Hugging Face의 GPU 지원 발표는 소규모 개발자와 스타트업에게 큰 도움이 될 것입니다. 또한, AI 커뮤니티는 항상 새로운 모델의 성능과 가능성에 큰 관심을 가지고 있으며, 이번 주에 발표된 여러 모델들은 그 기대를 충족시키는 것으로 보입니다. 하지만, Scarlett Johansson의 OpenAI 비판과 같은 이슈는 AI 윤리와 책임에 대한 중요한 토론을 계속 일깨워주는 사례로 남습니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **Triton과 CUDA의 한계와 협업**: 엔지니어들은 Triton 블록의 한계를 논의하며, 4096 요소 블록은 실행 가능하지만 8192 요소 블록은 실행 불가능함을 확인했습니다. 이는 예상 CUDA 한계와의 불일치를 시사합니다. ([Compiler Explorer](https://godbolt.org/z/9K9Gf1v6P?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **PyTorch와 LLM 추론 최적화**: PyTorch의 `linear` 함수와 행렬 곱셈 커널의 동작을 검토하고, Effort Engine 알고리즘이 LLM 추론 중 계산 노력을 조정할 수 있게 함으로써, Apple Silicon에서 표준 행렬 곱셈과 비슷한 속도를 낼 수 있음을 논의했습니다. ([Effort Engine on kolinko](https://kolinko.github.io/effort?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408), [GitHub](https://github.com/kolinko/effort?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **InstaDeep의 기계학습 엔지니어 채용**: InstaDeep은 고성능 ML 엔지니어링, 사용자 정의 CUDA 커널 및 분산 훈련에 능숙한 기계 학습 엔지니어를 채용 중입니다. ([InstaDeep Careers](https://www.instadeep.com/job-offer/92900fa3-5501-4506-a63f-cebee958fc6f/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))

**2.인공지능 기술의 향후 전망**
- **LLM의 컨텍스트 길이 증가**: Llama-3 8B 모델이 컨텍스트 길이 증가를 통해 새로운 벤치마크를 설정했습니다. 이는 LLM의 가능성을 크게 확장시키는 발전입니다. ([Llama-3 8B Gradient Instruct 1048k](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
- **ROCm과 Flash Attention 2의 적응**: ROCm 채널에서는 NVIDIA의 Flash Attention 2를 ROCm에 적용하는 논의가 있었습니다. 이는 ROCm 6.x 버전과의 호환성에 중점을 두고 있습니다. ([ROCm/flash-attention on GitHub](https://github.com/ROCm/flash-attention?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-4408))
    
