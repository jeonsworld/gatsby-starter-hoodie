---
title: "Apple의 OpenELM, OLMo를 50%의 데이터셋으로 압도하다: DeLighT 아키텍처의 혁신"
description: "Apple의 OpenELM, OLMo를 50%의 데이터셋으로 압도하다: DeLighT 아키텍처의 혁신"
date: 2024-04-27
update: 2024-04-27
tags:
  - OpenELM
  - DeLighT 아키텍처
  - AI 경쟁
---
   
Apple의 AI 분야에서의 발전은 [WWDC](https://buttondown.email/ainews/archive/ainews-mm1-apples-first-large-multimodal-model/)를 앞두고 계속되고 있습니다. 이전에 [OLMo](https://buttondown.email/ainews/archive/ainews-ai2-releases-olmo-the-4th-open-everything/)에 대해 다루었던 바와 같이, OpenELM은 Apple이 공개한 첫 번째 [실제로 개방형 LLM](https://arxiv.org/abs/2404.14619?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)입니다. 이 모델은 [가중치](https://huggingface.co/apple/OpenELM?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)와 [코드](https://github.com/apple/corenet?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)를 공유하며, 효율적인 아키텍처 방향으로 새로운 연구를 제시합니다. ![image](https://assets.buttondown.email/images/3bd4b772-df2f-46b7-8318-2cc230b7eb46.png?w=960&fit=max)

Sebastian Raschka는 이에 대해 [트위터에서 언급했습니다](https://twitter.com/rasbt/status/1783480053847736713/photo/1?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its). OpenELM은 완전히 개방형은 아니지만 상당히 개방적입니다. ![image](https://assets.buttondown.email/images/5a0bcc71-6f46-41a3-a34b-6efff203c64d.png?w=960&fit=max)

[DeLighT](https://arxiv.org/abs/2008.00623?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its) 논문에서는 표준 주의 메커니즘을 깊게 만들어 레이어의 수를 2.5-5배 증가시키면서도, 매개변수 수로는 2-3배 큰 모델과 동등한 성능을 내는 방법을 제시합니다. ![image](https://assets.buttondown.email/images/64a3ecf6-fbca-4816-9233-f4100454aca8.png?w=960&fit=max)
![image](https://assets.buttondown.email/images/a70b5ba1-00bb-482d-a4a4-f1027eec0266.png?w=960&fit=max)

## Today's Preview
* **LLaMA 개발**
  - LLaMA 3은 컨텍스트 길이를 160K+ 토큰으로 증가시키며 완벽한 리콜을 유지합니다. [더 알아보기](https://www.reddit.com/r/LocalLLaMA/comments/1ccqmjz/llama_3_now_with_160k_context/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)
  - LLaMA-3 8B-Instruct 모델이 262K 컨텍스트 길이로 처음 출시되었습니다. [자세히 보기](https://www.reddit.com/r/LocalLLaMA/comments/1cd4yim/llama38binstruct_with_a_262k_context_length/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)
* **AI 윤리 및 규제**
  - Eric Schmidt는 오픈 소스 AI 모델이 나쁜 행위자들과 중국에 위험한 능력을 제공한다고 경고합니다. [자세히 보기](https://www.reddit.com/r/singularity/comments/1ccyqkr/former_google_ceo_eric_schmidt_warns_that_open/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)
* **하드웨어 개발**
  - TSMC는 1.6nm 공정 노드를 발표했습니다. [더 알아보기](https://www.reddit.com/r/singularity/comments/1ccr4hy/tsmc_unveils_16nm_process_technology_with/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its)

### AI Viewpoint 🤖
> Apple의 OpenELM과 OLMo 비교는 AI 기술의 경쟁적 발전을 보여줍니다. 특히, OpenELM의 개방성과 DeLighT 아키텍처의 적용은 기존 모델보다 효율적인 방법으로 더 나은 성능을 제공할 수 있는 가능성을 시사합니다. 이러한 혁신은 머신러닝 분야에서 지속적인 연구와 개발의 중요성을 강조하며, 향후 AI 기술의 진화 방향에 중대한 영향을 미칠 것입니다.

## Today's Overview
### 최신 인공지능 기술과 응용 분야의 종합 리포트

**1. 최신 인공지능 기술 동향**
- **모델의 세밀한 조정과 도전**: 모델의 미세 조정에 관련된 도전들이 공유되었습니다. 예를 들어, 4비트 양자화, VRAM 요구 사항이 48GB를 초과하는 경우, 토큰 인덱스 시퀀스 길이 오류 등이 있습니다. 이러한 문제들에 대한 해결책으로는 배치 크기 수정, 팩 기능 활성화 및 **Aphrodite Engine**이나 **llama.cpp**와 같은 대안적 테스트 환경 고려가 포함됩니다. ([pack 기능](https://github.com/unslothai/unsloth?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its#-finetune-for-free))

- **WizardLM 팀 해체에 대한 소문**: **WizardLM** 팀의 해체에 대한 소문이 돌고 있습니다. **Qingfeng Sun**의 스태프 페이지 리디렉션과 관련된 논의가 있었습니다. 이에 대한 의견은 **WizardLM 데이터셋**을 구출하려는 시도부터 **Meta의 LlaMA-3 모델**을 포함한 쇼케이스 세션까지 다양했습니다.

- **Kolibrify의 오픈 소스 릴리스**: **Kolibrify**, 지시사항을 따르는 LLM을 위한 커리큘럼 훈련 도구가 오픈 소스로 공개되었습니다. 기술적인 측면에서는 **Triton** 의존성, "양자화 실패" 오류 및 **gguf 모델** 테스트 전략에 대한 논의가 있었습니다. ([Kolibrify GitHub](https://github.com/oKatanaaa/kolibrify?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its))

- **프루닝의 실용적 진전**: 모델을 사용하는 동안 평가 중에 작동하는 **triton laser merge trainer**에 대한 프로젝트 인사이트가 공유되었습니다. 이 방법은 시스템 개편 없이 모델 사용성을 향상시킬 수 있는 길을 제공할 수 있습니다. ([triton laser merge trainer](https://github.com/l4b4r4b4b4/trl/tree/evol_laser_merge_trainer?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its))

**2. 인공지능 기술의 향후 전망**
- **컨텍스트 길이 증가에 대한 모델의 접근**: Llama 3 모델은 PoSE와 지속적인 사전 훈련을 사용하여 96k 컨텍스트 길이를 달성했습니다. 이는 LLM의 가능성을 크게 확장시키는 발전입니다. ([PoSE 연구 논문](https://openreview.net/forum?id=3Z1gxuAQrA&utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its))

- **모델 훈련 효율성 증가**: 비전 모델 훈련에서 혁신적인 약한 감독 사전 훈련 방법이 도입되어 전통적인 대조 학습보다 2.7배 빠른 훈련이 가능하게 되었습니다. 이 접근 방식은 CLIP 모델과 비슷한 성능을 유지하면서도 훨씬 더 효율적입니다. ([연구 논문](https://arxiv.org/abs/2404.15653?utm_source=ainews&utm_medium=email&utm_campaign=ainews-apples-openelm-beats-olmo-with-50-of-its))

이러한 정보는 인공지능 기술의 최신 동향과 미래 전망을 이해하는 데 도움이 됩니다.
    
    